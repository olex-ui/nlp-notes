{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# read data from text files\n",
    "with open('./data/reviews.txt', 'r') as f:\n",
    "    reviews = f.read()\n",
    "with open('./data/labels.txt', 'r') as f:\n",
    "    labels = f.read()\n",
    "\n",
    "# type(reviews)\n",
    "# print(reviews[:1000])\n",
    "# print()\n",
    "# print(labels[:20])\n",
    "# type(\"hello\")\n",
    "# a=\"world\"\n",
    "# print(a[:3])\n",
    "from string import punctuation\n",
    "\n",
    "# get rid of punctuation\n",
    "reviews = reviews.lower() # lowercase, standardize\n",
    "all_text = ''.join([c for c in reviews if c not in punctuation])\n",
    "\n",
    "# split by new lines and spaces\n",
    "#一句一句话构成的list\n",
    "reviews_splitjuhua_list = all_text.split('\\n')\n",
    "all_text = ' '.join(reviews_splitjuhua_list)\n",
    "# len(reviews_splitjuhua_list)\n",
    "# create a list of words\n",
    "words = all_text.split()\n",
    "# words[:30]\n",
    "\n",
    "# feel free to use this import\n",
    "from collections import Counter\n",
    "\n",
    "## Build a dictionary that maps words to integers\n",
    "counts = Counter(words)\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab\n",
    "\n",
    "#独一无二的词+编号\n",
    "vocab_and_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
    "vocab_and_int\n",
    "## use the dict to tokenize each review in reviews_splitjuhua_list\n",
    "## store the tokenized reviews in reviews_anzhaojuzi_intsarray\n",
    "reviews_anzhaojuzi_intsarray = []\n",
    "for review in reviews_splitjuhua_list:\n",
    "    reviews_anzhaojuzi_intsarray.append([vocab_and_int[word] for word in review.split()])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words:  74072\n",
      "\n",
      "Tokenized review: \n",
      " [[21025, 308, 6, 3, 1050, 207, 8, 2138, 32, 1, 171, 57, 15, 49, 81, 5785, 44, 382, 110, 140, 15, 5194, 60, 154, 9, 1, 4975, 5852, 475, 71, 5, 260, 12, 21025, 308, 13, 1978, 6, 74, 2395, 5, 613, 73, 6, 5194, 1, 24103, 5, 1983, 10166, 1, 5786, 1499, 36, 51, 66, 204, 145, 67, 1199, 5194, 19869, 1, 37442, 4, 1, 221, 883, 31, 2988, 71, 4, 1, 5787, 10, 686, 2, 67, 1499, 54, 10, 216, 1, 383, 9, 62, 3, 1406, 3686, 783, 5, 3483, 180, 1, 382, 10, 1212, 13583, 32, 308, 3, 349, 341, 2913, 10, 143, 127, 5, 7690, 30, 4, 129, 5194, 1406, 2326, 5, 21025, 308, 10, 528, 12, 109, 1448, 4, 60, 543, 102, 12, 21025, 308, 6, 227, 4146, 48, 3, 2211, 12, 8, 215, 23]]\n",
      "[1 0 1 ... 1 0 0]\n",
      "Zero-length reviews: 1\n",
      "Maximum review length: 2514\n"
     ]
    }
   ],
   "source": [
    "# stats about vocabulary\n",
    "print('Unique words: ', len((vocab_and_int)))  # should ~ 74000+\n",
    "print()\n",
    "\n",
    "# print tokens in first review\n",
    "print('Tokenized review: \\n', reviews_anzhaojuzi_intsarray[:1])\n",
    "length=len(reviews_anzhaojuzi_intsarray)\n",
    "length\n",
    "# 1=positive, 0=negative label conversion\n",
    "labels_split = labels.split('\\n')\n",
    "encoded_labels = np.array([1 if label == 'positive' else 0 for label in labels_split])\n",
    "print(encoded_labels)\n",
    "# outlier review stats\n",
    "review_lens = Counter([len(x) for x in reviews_anzhaojuzi_intsarray])\n",
    "review_lens\n",
    "print(\"Zero-length reviews: {}\".format(review_lens[0]))\n",
    "print(\"Maximum review length: {}\".format(max(review_lens)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews before removing outliers:  25001\n",
      "25000\n",
      "Number of reviews after removing outliers:  25000\n"
     ]
    },
    {
     "data": {
      "text/plain": "25000"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Number of reviews before removing outliers: ', len(reviews_anzhaojuzi_intsarray))\n",
    "## remove any reviews/labels with zero length from the reviews_anzhaojuzi_intsarray list.\n",
    "\n",
    "# get indices of any reviews with length 0\n",
    "non_zero_idx = [ii for ii, review in enumerate(reviews_anzhaojuzi_intsarray) if len(review) != 0]\n",
    "len(reviews_anzhaojuzi_intsarray)\n",
    "# remove 0-length reviews and their labels\n",
    "reviews_anzhaojuzi_intsarray = [reviews_anzhaojuzi_intsarray[ii] for ii in non_zero_idx]\n",
    "encoded_labels = np.array([encoded_labels[ii] for ii in non_zero_idx])\n",
    "print(len(encoded_labels))\n",
    "print('Number of reviews after removing outliers: ', len(reviews_anzhaojuzi_intsarray))\n",
    "len(reviews_anzhaojuzi_intsarray)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0 ...     8   215    23]\n",
      " [    0     0     0 ...    29   108  3324]\n",
      " [22382    42 46418 ...   483    17     3]\n",
      " ...\n",
      " [   49   107    12 ...  1531    56   339]\n",
      " [    0     0     0 ...    81    95    38]\n",
      " [    0     0     0 ...    11     6  1321]]\n"
     ]
    }
   ],
   "source": [
    "#选择每个句子长为200\n",
    "seq_len = 200\n",
    "\n",
    "def pad_features(reviews_anzhaojuzi_intsarray, seq_length):\n",
    "    ''' Return features of review_ints, where each review is padded with 0's\n",
    "        or truncated to the input seq_length.\n",
    "    '''\n",
    "\n",
    "    # getting the correct rows x cols shape\n",
    "    features = np.zeros((len(reviews_anzhaojuzi_intsarray), seq_length), dtype=int)\n",
    "\n",
    "    # for each review, I grab that review and\n",
    "    for i, row in enumerate(reviews_anzhaojuzi_intsarray):\n",
    "        features[i, -len(row):] = np.array(row)[:seq_length]\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "# Test your implementation!\n",
    "\n",
    "seq_length = 200\n",
    "\n",
    "features = pad_features(reviews_anzhaojuzi_intsarray, seq_length=seq_length)\n",
    "\n",
    "## test statements - do not change - ##\n",
    "assert len(features)==len(reviews_anzhaojuzi_intsarray), \"Your features should have as many rows as reviews.\"\n",
    "assert len(features[0])==seq_length, \"Each feature row should contain seq_length values.\"\n",
    "\n",
    "# print first 10 values of the first 30 batches\n",
    "print(features)\n",
    "#ndarray型的m*n型数字矩阵"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(20000, 200) \n",
      "Train_Y set: \t(20000,) \n",
      "Test set: \t\t(5000, 200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-ddaa465405e9>:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  for train_index,test_index in ss.split(np.array(reviews_ints)):\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "ss = ShuffleSplit(n_splits=1,test_size=0.2,random_state=0)\n",
    "for train_index,test_index in ss.split(np.array(reviews_anzhaojuzi_intsarray)):\n",
    "    train_x = features[train_index]\n",
    "    train_y = encoded_labels[train_index]\n",
    "    test_x = features[test_index]\n",
    "    test_y = encoded_labels[test_index]\n",
    "\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape),\n",
    "      \"\\nTrain_Y set: \\t{}\".format(train_y.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[31,  5, 30, 73],\n       [17, 73, 96, 70]])"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a=lambda x:x*x\n",
    "# print(a(3))\n",
    "#\n",
    "# b=lambda x:x*2\n",
    "# b(20)\n",
    "#\n",
    "# dict={1:\"the\",2:\"and\"}\n",
    "#\n",
    "# students = [('john', 'A', 15), ('jane', 'B', 12), ('dave', 'B', 10)]\n",
    "# sorted(students, key=lambda s: s[2])\n",
    "\n",
    "list1=['a','b','c']\n",
    "list2=[1,2,3]\n",
    "dict1={a:a for a in list1}\n",
    "dict1\n",
    "\n",
    "print([i for i in range(1, 11)])#[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "a = [1, 2, 3, 4, 5]\n",
    "b = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "# 如何得出c = [\"a1\", \"b2\", \"c3\", \"d4\", \"e5\"]\n",
    "c=[str(x)+str(y) for x,y in zip(b,a)]\n",
    "c\n",
    "\n",
    "#if else列表表达式\n",
    "d=[0 if x%2==0 else x*2 for x in a]\n",
    "len(d)\n",
    "\n",
    "np.random.randint(0,100,size=(2,4),dtype=int)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(20000, 200) \n",
      "Validation set: \t(2500, 200) \n",
      "Test set: \t\t(2500, 200)\n"
     ]
    }
   ],
   "source": [
    "split_frac = 0.8\n",
    "\n",
    "## split data into training, validation, and test data (features and labels, x and y)\n",
    "\n",
    "split_idx = int(len(features)*split_frac)\n",
    "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
    "train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
    "\n",
    "test_idx = int(len(remaining_x)*0.5)\n",
    "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
    "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
    "\n",
    "## print out the shapes of your resultant feature data\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape),\n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 50\n",
    "\n",
    "# make sure the SHUFFLE your training data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# First checking if GPU is available\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentRNN(\n",
      "  (embedding): Embedding(74073, 400)\n",
      "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SentimentRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, bidirectional=True, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(SentimentRNN, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bidirectional = bidirectional\n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
    "                            dropout=drop_prob, batch_first=True,\n",
    "                            bidirectional=bidirectional)\n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)#以0.3的概率让某些神经元停止工作\n",
    "\n",
    "        # linear and sigmoid layers\n",
    "        if bidirectional:\n",
    "          self.fc = nn.Linear(hidden_dim*2, output_size)\n",
    "        else:\n",
    "          self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # embeddings and lstm_out\n",
    "        x = x.long()\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "\n",
    "#         if bidirectional:\n",
    "#           lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim*2)\n",
    "#         else:\n",
    "#           lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "\n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "\n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "\n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        number = 1\n",
    "        if self.bidirectional:\n",
    "           number = 2\n",
    "\n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers*number, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                      weight.new(self.n_layers*number, batch_size, self.hidden_dim).zero_().cuda()\n",
    "                     )\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers*number, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers*number, batch_size, self.hidden_dim).zero_()\n",
    "                     )\n",
    "\n",
    "        return hidden\n",
    "\n",
    "\n",
    "vocab_size = len(vocab_and_int)+1 # +1 for the 0 padding + our word tokens\n",
    "output_size = 1\n",
    "embedding_dim = 400\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "bidirectional = False  #这里为True，为双向LSTM\n",
    "\n",
    "net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, bidirectional)\n",
    "\n",
    "print(net)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/4... Step: 100... Loss: 0.624005... Val Loss: 0.644192\n",
      "Epoch: 1/4... Step: 200... Loss: 0.687304... Val Loss: 0.688116\n",
      "Epoch: 1/4... Step: 300... Loss: 0.656812... Val Loss: 0.664682\n",
      "Epoch: 1/4... Step: 400... Loss: 0.689003... Val Loss: 0.680248\n",
      "Epoch: 2/4... Step: 100... Loss: 0.578315... Val Loss: 0.617746\n",
      "Epoch: 2/4... Step: 200... Loss: 0.562378... Val Loss: 0.622129\n",
      "Epoch: 2/4... Step: 300... Loss: 0.563897... Val Loss: 0.469249\n",
      "Epoch: 2/4... Step: 400... Loss: 0.437930... Val Loss: 0.443221\n",
      "Epoch: 3/4... Step: 100... Loss: 0.441925... Val Loss: 0.452530\n",
      "Epoch: 3/4... Step: 200... Loss: 0.196485... Val Loss: 0.450632\n",
      "Epoch: 3/4... Step: 300... Loss: 0.242050... Val Loss: 0.429014\n",
      "Epoch: 3/4... Step: 400... Loss: 0.290332... Val Loss: 0.487103\n",
      "Epoch: 4/4... Step: 100... Loss: 0.259542... Val Loss: 0.491247\n",
      "Epoch: 4/4... Step: 200... Loss: 0.111814... Val Loss: 0.470559\n",
      "Epoch: 4/4... Step: 300... Loss: 0.196392... Val Loss: 0.511773\n",
      "Epoch: 4/4... Step: 400... Loss: 0.170949... Val Loss: 0.484157\n"
     ]
    }
   ],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "# training params\n",
    "\n",
    "epochs = 4 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
    "\n",
    "print_every = 100\n",
    "clip=5 # gradient clipping\n",
    "\n",
    "# move model to GPU, if available\n",
    "if(train_on_gpu):\n",
    "    net.cuda()\n",
    "\n",
    "net.train()\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "    counter = 0\n",
    "\n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "\n",
    "        if(train_on_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "\n",
    "        # get the output from the model\n",
    "        output, h = net(inputs, h)\n",
    "\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss stats\n",
    "        if counter % print_every == 0:\n",
    "            # Get validation loss\n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for inputs, labels in valid_loader:\n",
    "\n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                if(train_on_gpu):\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                output, val_h = net(inputs, val_h)\n",
    "                val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "            net.train()\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.481\n",
      "Test accuracy: 0.816\n"
     ]
    }
   ],
   "source": [
    "#测试测试测试\n",
    "# Get test data loss and accuracy\n",
    "\n",
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "\n",
    "# init hidden state\n",
    "h = net.init_hidden(batch_size)\n",
    "\n",
    "net.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "\n",
    "    # Creating new variables for the hidden state, otherwise\n",
    "    # we'd backprop through the entire training history\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "    # get predicted outputs\n",
    "    output, h = net(inputs, h)\n",
    "\n",
    "    # calculate loss\n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "\n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
    "\n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "\n",
    "# -- stats! -- ##\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3]) tensor(44)\n",
      "tensor([4, 5, 6]) tensor(55)\n",
      "tensor([7, 8, 9]) tensor(66)\n",
      "tensor([1, 2, 3]) tensor(44)\n",
      "tensor([4, 5, 6]) tensor(55)\n",
      "tensor([7, 8, 9]) tensor(66)\n",
      "tensor([1, 2, 3]) tensor(44)\n",
      "tensor([4, 5, 6]) tensor(55)\n",
      "tensor([7, 8, 9]) tensor(66)\n",
      "tensor([1, 2, 3]) tensor(44)\n",
      "tensor([4, 5, 6]) tensor(55)\n",
      "tensor([7, 8, 9]) tensor(66)\n",
      "================================================================================\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x00000242C6A60F40>\n",
      " batch:1 x_data:tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9],\n",
      "        [4, 5, 6]])  label: tensor([44, 55, 66, 55])\n",
      " batch:2 x_data:tensor([[4, 5, 6],\n",
      "        [1, 2, 3],\n",
      "        [7, 8, 9],\n",
      "        [1, 2, 3]])  label: tensor([55, 44, 66, 44])\n",
      " batch:3 x_data:tensor([[7, 8, 9],\n",
      "        [7, 8, 9],\n",
      "        [1, 2, 3],\n",
      "        [4, 5, 6]])  label: tensor([66, 66, 44, 55])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9], [1, 2, 3], [4, 5, 6], [7, 8, 9], [1, 2, 3], [4, 5, 6], [7, 8, 9], [1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "b = torch.tensor([44, 55, 66, 44, 55, 66, 44, 55, 66, 44, 55, 66])\n",
    "a\n",
    "# TensorDataset对tensor进行打包\n",
    "train_ids = TensorDataset(a, b)\n",
    "train_ids\n",
    "for x_train, y_label in train_ids:\n",
    "    print(x_train, y_label)\n",
    "\n",
    "# dataloader进行数据封装\n",
    "print('=' * 80)\n",
    "train_loader = DataLoader(dataset=train_ids, batch_size=4, shuffle=True)\n",
    "print(train_loader)\n",
    "for i, data in enumerate(train_loader, 1):\n",
    "# 注意enumerate返回值有两个,一个是序号，一个是数据（包含训练数据和标签）\n",
    "    x_data, label = data\n",
    "    print(' batch:{0} x_data:{1}  label: {2}'.format(i, x_data, label))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn=torch.nn.LSTM(\n",
    "            input_size=28,\n",
    "            hidden_size=64,\n",
    "            num_layers=1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.out=torch.nn.Linear(in_features=64,out_features=10)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # 以下关于shape的注释只针对单向\n",
    "        # output: [batch_size, time_step, hidden_size]\n",
    "        # h_n: [num_layers,batch_size, hidden_size] # 虽然LSTM的batch_first为True,但是h_n/c_n的第一维还是num_layers\n",
    "        # c_n: 同h_n\n",
    "        output,(h_n,c_n)=self.rnn(x)\n",
    "        print(output.size())\n",
    "        # output_in_last_timestep=output[:,-1,:] # 也是可以的\n",
    "        output_in_last_timestep=h_n[-1,:,:]\n",
    "        # print(output_in_last_timestep.equal(output[:,-1,:])) #ture\n",
    "        x=self.out(output_in_last_timestep)\n",
    "        return x\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. 加载数据\n",
    "    training_dataset = torchvision.datasets.MNIST(\"./mnist\", train=True,\n",
    "                                                  transform=torchvision.transforms.ToTensor(), download=True)\n",
    "    dataloader = Data.DataLoader(dataset=training_dataset,\n",
    "                                          batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "    # showSample(dataloader)\n",
    "    test_data=torchvision.datasets.MNIST(root=\"./mnist\",train=False,\n",
    "                                        transform=torchvision.transforms.ToTensor(),download=False)\n",
    "    test_dataloader=Data.DataLoader(\n",
    "        dataset=test_data,batch_size=1000,shuffle=False,num_workers=2)\n",
    "    testdata_iter=iter(test_dataloader)\n",
    "    test_x,test_y=testdata_iter.next()\n",
    "    test_x=test_x.view(-1,28,28)\n",
    "    # 2. 网络搭建\n",
    "    net=RNN()\n",
    "    # 3. 训练\n",
    "    # 3. 网络的训练（和之前CNN训练的代码基本一样）\n",
    "    optimizer=torch.optim.Adam(net.parameters(),lr=0.001)\n",
    "    loss_F=torch.nn.CrossEntropyLoss()\n",
    "    for epoch in range(1): # 数据集只迭代一次\n",
    "        for step, input_data in enumerate(dataloader):\n",
    "            x,y=input_data\n",
    "            pred=net(x.view(-1,28,28))\n",
    "\n",
    "            loss=loss_F(pred,y) # 计算loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if step%50==49: # 每50步，计算精度\n",
    "                with torch.no_grad():\n",
    "                    test_pred=net(test_x)\n",
    "                    prob=torch.nn.functional.softmax(test_pred,dim=1)\n",
    "                    pred_cls=torch.argmax(prob,dim=1)\n",
    "                    acc=(pred_cls==test_y).sum().numpy()/pred_cls.size()[0]\n",
    "                    print(f\"{epoch}-{step}: accuracy:{acc}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "myenv",
   "language": "python",
   "display_name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}